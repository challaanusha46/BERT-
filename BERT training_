{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGNIa0Cb2zjVW8Eb91Klic"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install transformers"],"metadata":{"id":"bFjjEGVEFvad","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690257326983,"user_tz":240,"elapsed":6376,"user":{"displayName":"Anusha Challa","userId":"08483865548712783808"}},"outputId":"763d24a4-21d6-4277-d4cf-f8b2399f2f50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertTokenizer, BertModel, AdamW"],"metadata":{"id":"1xJuYSgG_9qx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming you have the unsupervised dataset of student responses\n","student_responses = \"\\Downloads\\SciEd_BERT.xls\"  # Replace this with your student response data\n","\n","# Load the BERT model\n","model_name = 'bert-base-uncased'\n","model = BertModel.from_pretrained(model_name)"],"metadata":{"id":"T4DZQ67SKj3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UYZ29OeYT0dp","executionInfo":{"status":"ok","timestamp":1690258883725,"user_tz":240,"elapsed":10777,"user":{"displayName":"Anusha Challa","userId":"08483865548712783808"}},"outputId":"dc35dc84-ae82-411e-a3f0-6a64996b19a7"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Before if path\n"]}],"source":["import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertTokenizer, BertForMaskedLM, AdamW\n","import os\n","\n","# Assuming you have the unsupervised dataset of student responses\n","# student_responses = [...]  # Replace this with your student response data\n","\n","# Load the BERT model for masked language modeling\n","model_name = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","model = BertForMaskedLM.from_pretrained(model_name)  # Load BERT model for MLM\n","\n","# Create a custom dataset class for the unsupervised data\n","class UnsupervisedDataset(torch.utils.data.Dataset):\n","    def __init__(self, student_responses):\n","        self.student_responses = student_responses\n","\n","    def __getitem__(self, index):\n","        response = self.student_responses[index]\n","        return response\n","\n","    def __len__(self):\n","        return len(self.student_responses)\n","\n","# Prepare the data loader for unsupervised learning\n","batch_size = 16\n","unsupervised_dataset = UnsupervisedDataset(student_responses)\n","unsupervised_dataloader = DataLoader(unsupervised_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Configure the optimizer and train the BERT model\n","learning_rate = 2e-5\n","num_epochs = 3\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","no_deprecation_warning=True\n","\n","for epoch in range(num_epochs):\n","    for batch in unsupervised_dataloader:\n","        optimizer.zero_grad()\n","        responses = batch\n","        inputs = tokenizer(responses, padding=True, truncation=True, return_tensors='pt')\n","        outputs = model(**inputs, labels=inputs.input_ids)  # Use the input_ids as labels for MLM\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","# Save the trained BERT model\n","#saved_model_path = \"/Downloads/unsupervised_bert_model\"\n","# model.save_pretrained(saved_model_path)\n","\n","user_home_dir = os.path.expanduser(\"~\")\n","downloads_dir = os.path.join(user_home_dir, \"Downloads\")\n","saved_model_path = os.path.join(downloads_dir, \"unsupervised_bert_model\")\n","model.save_pretrained(saved_model_path)\n","print(\"Before if path\")\n","if not os.path.exists(downloads_dir):\n","   os.makedirs(downloads_dir)\n","   print(\"Path exists for saving up trained BERT model\")\n"]}]}